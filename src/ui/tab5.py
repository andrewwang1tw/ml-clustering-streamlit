import io
import json
import streamlit as st
import pandas as pd
from agent.langGraph_gemini import stream_graph_updates2, stream_graph_updates
from ui.ui_main import get_cluster_labels


#-----------------------------------------------------------------------------------
# Tab 5, show_cluster_details
#-----------------------------------------------------------------------------------
def tab5_show_cluster_details(df_clustered, page_size=10):    
    #st.subheader('ğŸ—“ï¸ ç¾¤é›†è©³ç´°æª¢è¦–')
    print("\ndf_clustered, Tab 5 ===>\n", df_clustered)    
        
    # --- æ¯ä¸€å€‹ cluster çš„ç­†æ•¸ ---   
    cluster_counts, cluster_labels = get_cluster_labels(df_clustered)
    col1, col2 = st.columns([2, 1])
    with col1: 
        selected = st.selectbox('ğŸ“ˆ é¸æ“‡ç¾¤é›†', options=cluster_labels, key="cluster_select_label")
    
    # --- æ‰¾å‡ºæ‰€é¸çš„ cluster ---
    idx = cluster_labels.index(selected)
    cluster_index = cluster_counts.index[idx]
    df_cluster_selected = df_clustered[df_clustered['cluster'] == cluster_index]
    print("\ndf_cluster_selected, Tab 5 ===>\n", df_cluster_selected)

    total = len(df_cluster_selected)
    total_pages = (total - 1) // page_size + 1 if total > 0 else 1         
    with col2: 
        page = st.number_input("ğŸ“ˆ é ç¢¼", min_value=1, max_value=total_pages, value=1, step=1, key="cluster_page_num")    
        
    start = (page - 1) * page_size
    end = start + page_size        
    st.write(f'ğŸ“ˆ ç¾¤ {cluster_index} (count={total}) - ç¬¬ {page}/{total_pages} é ')
    
    df = df_cluster_selected[['stock_id_','stock_name_', 
                    'continuous_flag',
                    'avg_mom','std_mom', 
                    'avg_yoy','std_yoy']].iloc[start:end].copy()    
     
    selected_rows_dict = st.dataframe(
        df,
        hide_index=True,
        on_select="rerun",
        selection_mode="single-row"        
    )   
    
    # --- æä¾›ä¸‹è¼‰è©²ç¾¤ Excel ---
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        df_cluster_selected.to_excel(writer, index=False, sheet_name=f'cluster_{cluster_index}')
        
    excel_data = buffer.getvalue()
    st.download_button(
        label="ğŸ“ˆ ä¸‹è¼‰è©²ç¾¤çµæœ Excel",
        data=excel_data,
        file_name=f"cluster_{cluster_index}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    
    # --- å–å¾—é¸å–åˆ°çš„è³‡æ–™, å¦‚æœæœ‰ä»»ä½•ä¸€è¡Œè¢«é¸å– ---
    selected_indices = selected_rows_dict.get('selection', {}).get('rows', [])
    if selected_indices:
        # å–å¾—è¢«é¸å–çš„åˆ—çš„ç´¢å¼•
        print("\né¸å–åˆ°çš„è³‡æ–™, selected_indices ===>\n", selected_indices, "\n")
        selected_row_index = selected_indices[0]       
        selected_row_data = df.iloc[selected_row_index]        
        print("\né¸å–åˆ°çš„è³‡æ–™m selected_row_data ===>\n", selected_row_data, "\n")
        
        # å–å¾—ç‰¹å®šæ¬„ä½çš„å€¼
        selected_id = selected_row_data['stock_id_']
        selected_name = selected_row_data['stock_name_']
        print("\nå–å¾—é¸å–åˆ°çš„è³‡æ–™ ===>\nselected_id= ", selected_id, ", selected_name= ", selected_name, "\n")
        
        # --- Use LangGraph --- 
        user_input = f"""
            Summary Taiwan stock {str(selected_id)}.TW for the most recent 6 months basic information, stock price, company operation and related news.             
            Include domain will be https://tw.stock.yahoo.com/ , https://www.cnyes.com/ , https://www.ctee.com.tw/ and https://money.udn.com/ .            
            If you can not find related information just say "no information found!" directly.             
            Finally, must answer in Traditional Chinese.                                              
            """  
            
        # with st.status("ğŸŒ LangGraph æœå°‹ä¸­...", expanded=True) as status:              

        #     # 2. å»ºç«‹ä¸€å€‹èŠå¤©è¨Šæ¯å€å¡Šï¼Œé¡¯ç¤º AI çš„å›æ‡‰
        #     with st.chat_message("assistant"):
        #         # st.write_stream æœƒæ¥æ”¶ stream_graph_updates ç”¢å‡ºçš„å…§å®¹ä¸¦å³æ™‚é¡¯ç¤º
        #         # response_generator æ˜¯æˆ‘å€‘ä¸Šé¢ä¿®æ”¹å¾Œçš„ generator
        #         response_generator = stream_graph_updates2(user_input)
                
        #         # *** é—œéµä¿®æ”¹ï¼šä½¿ç”¨ st.write_stream è™•ç† generator ***
        #         full_response = st.write_stream(response_generator)
                
        #     # 3. ç‹€æ…‹å®Œæˆ (å¯ä»¥é¸æ“‡æ€§åœ°éš±è— statusï¼Œæˆ–ä¿æŒé¡¯ç¤º)
        #     status.update(label="ğŸ“‚ AI è§£é‡‹", state="complete", expanded=True)
            
        #     # å‚™è¨»ï¼šå¦‚æœä½ éœ€è¦ä¿ç•™å®Œæ•´çš„ full_response (ä¾‹å¦‚ç”¨æ–¼å¿«å–)ï¼Œ
        #     # st.write_stream() æœƒå›å‚³æ‰€æœ‰ç‰‡æ®µæ‹¼æ¥èµ·ä¾†çš„å®Œæ•´å­—ä¸²ã€‚
            
        with st.status("ğŸŒ LangGraph æœå°‹ä¸­...", expanded=True) as status:              
            all_messages, last_msg = stream_graph_updates(user_input)                               
            if all_messages:
                status.update(label="ğŸŒ LangGraph ä»»å‹™å·²å®Œæˆï¼", state="complete", expanded=True)     
                for message in all_messages: 
                    try:        
                        data = json.loads(message)  
                        if data:
                            with st.expander("ğŸ“‚ AI Agent é€é TavilySearch æœå°‹åˆ°çš„è³‡æ–™"):                                     
                                for result in data.get('results', []):
                                    with st.container():
                                        st.markdown(f"**[{result.get('title', 'ç„¡æ¨™é¡Œ')}]({result.get('url', '#')})**")
                                        st.markdown(f"*{result.get('content', 'ç„¡æ‘˜è¦')}*")
                                    
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œå°±ç•¶ä½œæ™®é€šæ–‡å­—é¡¯ç¤º    
                        with st.expander("ğŸ“¦ AI Agent ç¸½çµæœå°‹åˆ°çš„è³‡æ–™"):
                            st.write(message)
        
    